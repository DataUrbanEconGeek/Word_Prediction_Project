---
title: "NLP Project: Word Prediction Part 1"
author: "Andrew Rosa"
date: "April 9, 2016"
output:
        html_document:
                includes:
                        in_header: head.html
                css: style.css
---

##Executive Summary

In this Project we'll use three sets of data to create a word prediction application. The sets are from twitter, various blogs and news sources. They have been provided by SwiftKey through Coursera.com. In Part 1 we'll perform an exploritory analysis of the data. In Part 2 we'll create a predictive model and design an application. First I'll set the enviroment to the project's directory. 

```{r}
setwd("~/Desktop/Word_Prediction_Project")
root <- "~/Desktop/Word_Prediction_Project/"
```

### Download and Load Data Sets

We're going to need to download the zip file to the directory for the Coursera website, and then unzip it. This file contains data sets in several different languages. We're going to use the data sets that are in English for this project. This is located in the "en_US" directory. 

```{r results='asis', cache=TRUE}
url <- c("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip")
download.file(url, "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip", exdir = ".")
```

```{r echo=FALSE, cache=TRUE}
# Romove unused files from directory to save memory
unlink(paste0(root, "final/de_DE"), recursive = TRUE)
unlink(paste0(root, "final/fi_FI"), recursive = TRUE)
unlink(paste0(root, "final/ru_RU"), recursive = TRUE)
```

The three files are in text format. We need to use the `readLines` function to load the data into R.

```{r cache=TRUE, warning=FALSE}
twitter <- readLines(paste0(root, "final/en_US/en_US.twitter.txt"))
blogs <- readLines(paste0(root, "final/en_US/en_US.blogs.txt"))
news <- readLines(paste0(root, "final/en_US/en_US.news.txt"))
```

## Exploritory Analysis

### Summarise Data Sets

It's time to get a basic idea oh how the data looks. We're going to get a look at some of the information about these files, such as the size of the file and average word count with the use of a function that we'll create. This function will return a data frame that summarizes the data. 

```{r cache=TRUE, warning=FALSE, message=FALSE}
library(dplyr)
library(stringi)
summaryStats <- function(x){
        size <- round(file.info(paste0(root, "final/en_US/en_US.", deparse(substitute(x)), 
                                       ".txt"))$size / 1024^2)
        stats_1 <- stri_stats_general(x)
        words <- sum(stri_count_words(x))
        ave_words_per_line <- words/stats_1[1]
        maxwords <- max(stri_count_words(x))
        chars_per_word <- stats_1[3]/words
        data.frame(
                Source = c(paste0(deparse(substitute(x)))),
                Size_MB = c(size),
                Total_Lines = c(stats_1[1]),
                Total_Words = c(words),
                Total_Chars = c(stats_1[3]),
                Ave_Words_Per_Line = c(ave_words_per_line),
                Max_Words_Per_Line = c(maxwords),
                Chars_Per_Word = c(chars_per_word)
        )
}

twitter_summary <- summaryStats(twitter)
news_summary <- summaryStats(news)
blogs_summary <- summaryStats(blogs)

data_stats <- rbind(twitter_summary, news_summary)
data_stats <- rbind(data_stats, blogs_summary)
data_stats <- arrange(data_stats, desc(Size_MB))
```

```{r cache=TRUE, results='asis', echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
kable(data_stats)
```

We can see from the table the blogs data set is by far the largest file. This data set also contains the most number of words, characters, and words per line, but has the least amount of lines. It's the twitter data set that has the most amount of lines. The three data sets are very large. Running statistical analysis on these sets would take a lot of computational power and a lot of time. We're going to take random samples from we set that are much smaller in size in order to compute things at a quicker speed. First though, let's make sure that each set has a word count that is a little more even, as we don't want there to be any bias between the scources. We'll do this by deleting out some lines from two of the blogs and news scources. In worder to figure out how many lines need to be deleted we're going to subtract the blogs and news tottal word counts from the smallest wordcount(twitter with 30093369 words) then we'll divide it by the avarage number of words per lines.   

```{r cache=TRUE}
blog_delete <- (data_stats$Total_Words[1] - data_stats$Total_Words[3]) / 
        data_stats$Ave_Words_Per_Line[1]
news_delete <- (data_stats$Total_Words[2] - data_stats$Total_Words[3]) / 
        data_stats$Ave_Words_Per_Line[2]
blogs_2 <- blogs[1:(length(blogs) - round(blog_delete))]
news_2 <- news[1:(length(news) - round(news_delete))]
```

```{r, cache=TRUE, echo=FALSE}
news_summary_2 <- summaryStats(news_2)
blogs_summary_2 <- summaryStats(blogs_2)

data_stats_2 <- rbind(twitter_summary, news_summary_2)
data_stats_2 <- rbind(data_stats_2, blogs_summary_2)
data_stats_2 <- arrange(data_stats_2, desc(Source))
kable(data_stats_2)
```

### Sampling

Now that the word counts are much closer is size we're going to randomize each data set. Then we'll save the data sets to the project's directory.

```{r cache=TRUE}
set.seed(227)

randomizeSets <- function(data_set){
        rows <- sample(NROW(data_set))
        data_set_new <- data_set[rows]
        connection <- file(description = paste(root, 
                                 deparse(substitute(data_set)),
                                 "_randomized", ".txt", sep=""), 
                           open = "w")
        writeLines(data_set_new, con = connection)
        close(connection)
        return(data_set_new)
}

twitter_random <- randomizeSets(data_set = twitter)
blog_random <- randomizeSets(data_set = blogs_2)
news_random <- randomizeSets(data_set = news_2)
```

Next we'll grab the samples. 

```{r cache=TRUE}
sampleExtrack <- function(dataSet, size){
        split <- round(length(dataSet) * size)
        samp <- dataSet[1:split]
        return(samp)
}

twitter_sample <- sampleExtrack(twitter_random, size = .02)
blog_sample <- sampleExtrack(blog_random, size = .02)
news_sample <- sampleExtrack(news_random, size = .02)
```

### Cleanning and Term Frequencies 

Now that we have made samples of each set as well as a combined set of the samples we need to do a little cleaning and reformatting. The function bellow uses the `tm` package to easily clean the sets. This function will change all letters to lowercase for the sake of consistency, it will also remove any punctuation and white space. Once the sets are clean they are turned into document term matrices. Then, we can do some further exploratory analysis to see which words are the most frequently used in the data sets. We should also compare the frequencies of the words from one data set to another, and establish if the three set corelate with eachother. 

```{r cache=TRUE, cache.lazy=FALSE, warning=FALSE, message=FALSE}
library(tm)
library(slam)
library(stringr)

cleanUp <- function(x){
        Sample_Source <- VectorSource(x)
        Sample_Corpus <- VCorpus(Sample_Source)
        Sample_Clean <- tm_map(Sample_Corpus, content_transformer(tolower))
        Sample_Clean <- tm_map(Sample_Clean, removePunctuation)
        Sample_Clean <- tm_map(Sample_Clean, stripWhitespace)
        return(Sample_Clean)
}

matrixDTM <- function(x){
        cleaned <- cleanUp(x)
        dtm <- DocumentTermMatrix(cleaned)
        mtx <- as.matrix(dtm)
        return(mtx)
}

wordFreq <- function(x){
        mtx <- matrixDTM(x)
        sums <- colSums(mtx)
        sorted <- sort(sums, decreasing = TRUE)
        df <-  as.data.frame(sorted)
        names(df) <- str_split(deparse(substitute(x)), "_")[[1]][1]
        return(df)
}

twitter_freq <- wordFreq(twitter_sample)
blogs_freq <- wordFreq(blog_sample)
news_freq <- wordFreq(news_sample)
```

#### Top Words

```{r cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
library(plyr)
twitter_freq$word <- row.names(twitter_freq)
blogs_freq$word <- row.names(blogs_freq)
news_freq$word <- row.names(news_freq)

frequency_df <- join_all(list(twitter_freq[1:5000, , drop = FALSE], 
                              blogs_freq[1:5000, , drop = FALSE],
                              news_freq[1:5000, , drop = FALSE]), by = "word", type = "full")
frequency_df <- select(frequency_df, word, twitter, blog, news)


library(wordcloud)
par(mfrow = c(1, 3))

twitter_pal <- brewer.pal(9,"YlGnBu")
twitter_pal <- twitter_pal[(4:8)]

blog_pal <- brewer.pal(9,"PuRd")
blog_pal <- blog_pal[(5:9)]

news_pal <- brewer.pal(9,"OrRd")
news_pal <- news_pal[(5:9)]

twitter_cloud <- wordcloud(frequency_df$word[1:50], frequency_df$twitter[1:50], 
          scale = c(5,1), min.freq = 202, 
          colors = twitter_pal, rot.per=0.35, use.r.layout=FALSE, random.order = FALSE)

blog_cloud <- wordcloud(frequency_df$word[1:50], frequency_df$blog[1:50], 
          scale = c(5,1), min.freq = 202, 
          colors = blog_pal, rot.per=0.35, use.r.layout=FALSE, random.order = FALSE)

news_cloud <- wordcloud(frequency_df$word[1:39], frequency_df$news[1:39], 
          scale = c(5,1), min.freq = 202, 
          colors = news_pal, rot.per=0.35, use.r.layout=FALSE, random.order = FALSE)
```

```{r echo=FALSE, cache=TRUE}
kable(head(frequency_df))
```

As you can see from the table above the word "the" is use the most often in all the sets. This dose not surprise me. 

Now if we plot the sets against each other we can see that there is a strong correlation amounts words in all the sets. Meaning that words that often appear in one set will also often appear in another set. 
```{r cache=TRUE, warning=FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)
plot1 <- qplot(twitter, blog, data = frequency_df, main = "Twitter vs. Blogs Frequency of Words")
plot2 <- qplot(twitter, news, data = frequency_df, main = "Twitter vs. News Frequency of Words")
plot3 <- qplot(news, blog, data = frequency_df, main = "News vs. Blogs Frequency of Words")
grid.arrange(plot1, plot2, plot3, nrow = 2, ncol = 2)
```

### Combine Sources

Now lets combine the three seperate sources. 

```{r cache=TRUE}
master_set <- c(twitter_sample, blog_sample, news_sample)

master_random <- randomizeSets(master_set)
master_sample <- sampleExtrack(master_random, size = .1)

master_cleaned <- cleanUp(master_sample)
```

### Bigram and Trigram Tokens

Now that we have an idea of which words are used the most frequently overall we should try to get an idea of which combination of words also frequently appear in the data. From here on out we'll just be using the combined data set for ease. In order for us to get an idea of which combination of words are use we need to create word tokens. We'll make tokens for bi-grams(two word combinations), and trig-rams(three word combinations). The functions bellow create the bi-grams and trig-rams and plot out histograms of the most frequent combinations. 

```{r cache=TRUE, cache.lazy=FALSE, warning=FALSE, message=FALSE}
library(RWeka)
options(mc.cores = 1)

bigram_tokenizer <- function(x) 
        unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
bigram_tdm <- TermDocumentMatrix(master_cleaned, control = list(tokenize = bigram_tokenizer))
bigram_matrix <- as.matrix(bigram_tdm)

bigram_frequency <- rowSums(bigram_matrix)
bigram_frequency <- sort(bigram_frequency, decreasing = TRUE)

bigram_top_10 <- bigram_frequency[1:10]
bigram <- data.frame(bigram_top_10)
bigram$terms <- rownames(bigram)
rownames(bigram) <- NULL
colnames(bigram) <- c("frequency", "terms")

bigram_plot <- ggplot(bigram, aes(terms, frequency)) + 
        geom_bar(stat = "identity") + 
        coord_flip()
bigram_plot
```

```{r cache=TRUE, cache.lazy=FALSE}
trigram_tokenizer <- function(x)
        unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
trigram_tdm <- TermDocumentMatrix(master_cleaned, control = list(tokenize = trigram_tokenizer))
trigram_matrix <- as.matrix(trigram_tdm)

trigram_frequency <- rowSums(trigram_matrix)
trigram_frequency <- sort(trigram_frequency, decreasing = TRUE)

trigram_top_10 <- trigram_frequency[1:10]
trigram <- data.frame(trigram_top_10)
trigram$terms <- rownames(trigram)
rownames(trigram) <- NULL
colnames(trigram) <- c("frequency", "terms")

trigram_plot <- ggplot(trigram, aes(terms, frequency)) + 
        geom_bar(stat="identity") + 
        coord_flip()
trigram_plot
```


From this you can see that combination of words like "of the", "in the", "one of the", and "a lot of" are used often. 

## Conclusion

From this analysis we see which terms and combination of terms are used most often in the data sets. This will come in handy in making a text predictor application. After running the analysis, I have found that the samples may still be too large to quickly run prediction models of. A smaller sample may have to be made. 












